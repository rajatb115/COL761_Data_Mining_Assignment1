{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatb115/COL761_Data_Mining_Assignment1/blob/main/Part_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIGhsJaJEzAL",
        "outputId": "4cc6cb88-9a21-4dce-f3e7-0f9ca337b603"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ5I0_DfBW6O",
        "outputId": "b3fbe5aa-d7cf-41ae-a545-9ca906529431"
      },
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-+.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=282114 sha256=613bbef9c7a5d38f3b16945b61d7b8bfd4d7f2f1bc0bd20843b847cf8aa91f41\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-+.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.12.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 846 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl size=505740 sha256=13123dc6246062f1182b0aa15eb924e8c41da5f345629a6e8bf0de24db92e240\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/e2/2f/44956c61e3299573ffe12da9d1374c7576ca0c5fb1fe1ed38c\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-+.html\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.5.9.tar.gz (38 kB)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl size=317953 sha256=1c69fb74c2a4bb66b3c99e62ccea579f729c7cebde24d4c2453622e69d02c2e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/60/d8/8bb27f58d8578ba8046f7ea0aadbae89a731db884a644ba361\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-+.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading torch_spline_conv-1.2.1.tar.gz (13 kB)\n",
            "Building wheels for collected packages: torch-spline-conv\n",
            "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl size=133838 sha256=092a3fd96281d54658d15b35e91ecdea51442523807609e84d31ce5f8404d2da\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/33/73/780370b7c7bdf2340c0a7b971e915643f14795b4caa7a9a31f\n",
            "Successfully built torch-spline-conv\n",
            "Installing collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-+.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=12f2af2f99c632bea60d52a469b1356bc941095129537104bf21f09b7143f5ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHRjIZ_rE_Zb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Z6Ujyih5zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03db2003-860d-40c1-d465-73b05eb4811f"
      },
      "source": [
        "dataset = Planetoid(\"\",\"Cora\",num_train_per_class= 150,split= 'random' ,num_test = 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5iXE3-dhcx6"
      },
      "source": [
        "edges = dataset[0].edge_index\n",
        "vertices = dataset[0].train_mask.shape[0]\n",
        "edges = edges.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrP0wj8hgzD"
      },
      "source": [
        "adjacency = np.zeros((vertices,vertices),dtype=np.float)\n",
        "for edge in edges:\n",
        "    adjacency[edge[0]][edge[1]] = 1.00\n",
        "    adjacency[edge[1]][edge[0]] = 1.00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf6-TxPqhjBr"
      },
      "source": [
        "diag = np.diag(np.sum(adjacency,axis=0))\n",
        "Transition = np.dot(np.linalg.inv(diag),adjacency)\n",
        "\n",
        "def matexpo(mat,power):\n",
        "    result = np.identity(mat.shape[0])\n",
        "    while (power!=0):\n",
        "        result = np.dot(result,mat)\n",
        "        power -= 1\n",
        "    return result\n",
        "\n",
        "Transition_fin = matexpo(Transition,0)+ matexpo(Transition,1) + matexpo(Transition,2) + matexpo(Transition,3)+ matexpo(Transition,4)\n",
        "Transition_fin = Transition_fin / 5.00"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk8PN0ZhlEz"
      },
      "source": [
        "#Preparation of train_data\n",
        "x_train = []\n",
        "y_train = []\n",
        "train_vertices = np.where(dataset[0].train_mask==True)\n",
        "for i in range(0,len(train_vertices[0])):\n",
        "    remaining= np.delete(train_vertices[0],i)\n",
        "    store_vec = np.zeros(vertices)\n",
        "    store_vec[train_vertices[0][i]]=1 \n",
        "    distances = np.dot(Transition_fin,store_vec).reshape(-1)\n",
        " #   non_zero_sample = np.where(distances>0)\n",
        "#    zero_sample =  np.where(distances==0)\n",
        " #   random_sample = np.concatenate((np.random.choice(non_zero_sample[0],5),np.random.choice(zero_sample[0],5)))\n",
        "    for j in range(0,len(train_vertices[0])):\n",
        "        x_train.append([train_vertices[0][i],train_vertices[0][j]])\n",
        "        y_train.append(distances[train_vertices[0][j]])\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0i4ZR5TzN0a"
      },
      "source": [
        "#Preparation of valid_data\n",
        "x_val = []\n",
        "y_val = []\n",
        "val_vertices = np.where(dataset[0].val_mask==True)\n",
        "for i in range(0,len(val_vertices[0])):\n",
        "    remaining= np.delete(val_vertices[0],i)\n",
        "    store_vec = np.zeros(vertices)\n",
        "    store_vec[val_vertices[0][i]]=1 \n",
        "    distances = np.dot(Transition_fin,store_vec).reshape(-1)\n",
        "    non_zero_sample = np.where(distances>0)\n",
        "    zero_sample =  np.where(distances==0)\n",
        "    random_sample = np.concatenate((np.random.choice(non_zero_sample[0],5),np.random.choice(zero_sample[0],5)))\n",
        "    for j in range(0,10):\n",
        "        x_val.append([val_vertices[0][i],random_sample[j]])\n",
        "        y_val.append(distances[random_sample[j]])\n",
        "\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Bkw2fnho1S"
      },
      "source": [
        "#Preparation of test_data\n",
        "x_test = []\n",
        "y_test = []\n",
        "test_vertices = np.where(dataset[0].test_mask==True)\n",
        "for i in range(0,len(test_vertices[0])):\n",
        "    remaining= np.delete(test_vertices[0],i)\n",
        "    store_vec = np.zeros(vertices)\n",
        "    store_vec[test_vertices[0][i]]=1 \n",
        "    distances = np.dot(Transition_fin,store_vec).reshape(-1)\n",
        "    non_zero_sample = np.where(distances>0)\n",
        "    zero_sample =  np.where(distances==0)\n",
        "    random_sample = np.concatenate((np.random.choice(non_zero_sample[0],5),np.random.choice(zero_sample[0],5)))\n",
        "    for j in range(0,10):\n",
        "        x_test.append([test_vertices[0][i],random_sample[j]])\n",
        "        y_test.append(distances[random_sample[j]])\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ5ZPp6khq6n"
      },
      "source": [
        "from torch_geometric.nn import GATConv,GATv2Conv\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \n",
        "    def __init__(self,hidden_dim,output_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATv2Conv(in_channels= -1, out_channels= 500,heads=4,concat=False)\n",
        "        self.conv2 = GATv2Conv(in_channels= 500, out_channels= 250)      \n",
        "    def forward(self,data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "   \n",
        "class combined_model(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = GAT(200,100)\n",
        "        self.mlp1 = nn.Linear(input_dim,hidden_dim)\n",
        "        self.mlp2 = nn.Linear(hidden_dim,1)\n",
        "\n",
        "    def forward(self,data,x):\n",
        "        embeddings = self.layer1(data)\n",
        "        x1 = torch.index_select(embeddings,0,x.T[0].flatten())\n",
        "        y1 = torch.index_select(embeddings,0,x.T[1].flatten())\n",
        "        store = torch.cat((x1,y1),dim=1)\n",
        "        store = self.mlp1(store)\n",
        "        store = F.relu(store)\n",
        "        store = self.mlp2(store)\n",
        "        store = F.relu(store)\n",
        "        return store\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_fin = combined_model(500,400)\n",
        "model_fin = model_fin.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLUvabRuhtg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fa928a4-47e2-4044-ab5e-7a80b1825a48"
      },
      "source": [
        "#Training and testing\n",
        "data1 = dataset[0].to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_fin.parameters(), lr=0.0001,weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5, verbose=True)\n",
        "\n",
        "\n",
        "epoch = 200\n",
        "x_train_fin = torch.from_numpy(x_train)\n",
        "x_train_fin = x_train_fin.to(device)\n",
        "y_train_fin = torch.from_numpy(y_train)\n",
        "y_train_fin = y_train_fin.to(device)\n",
        "\n",
        "x_val_fin = torch.from_numpy(x_val)\n",
        "x_val_fin = x_val_fin.to(device)\n",
        "y_val_fin = torch.from_numpy(y_val)\n",
        "y_val_fin = y_val_fin.to(device)\n",
        "\n",
        "x_test_fin = torch.from_numpy(x_test)\n",
        "x_test_fin = x_test_fin.to(device)\n",
        "y_test_fin = torch.from_numpy(y_test)\n",
        "y_test_fin = y_test_fin.to(device)\n",
        "\n",
        "val_loss = None\n",
        "val_acc = []\n",
        "best_model = None\n",
        "patience_count= 0\n",
        "\n",
        "for i in range(0,epoch):\n",
        "    pred_val = model_fin(data1,x_train_fin).flatten()\n",
        "    y_train_fin= y_train_fin.float()\n",
        "    loss1 = loss_func(pred_val,y_train_fin)\n",
        "    print(f\"Value of training loss for epoch {i} is: \",loss1.item())  \n",
        "    optimizer.zero_grad()  \n",
        "    loss1.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    \n",
        "    pred_val = model_fin(data1,x_val_fin).flatten()\n",
        "    y_val_fin = y_val_fin.float()\n",
        "    loss1 = loss_func(pred_val,y_val_fin)\n",
        "    val_acc.append(loss1.item())\n",
        "    print(f\"Value of validation loss for epoch {i} is: \",loss1.item())    \n",
        "    if best_model is None:\n",
        "        best_model = model_fin\n",
        "        val_loss = loss1.item()\n",
        "        patience_count = 0\n",
        "\n",
        "    elif loss1.item() < val_loss:\n",
        "        best_model = model_fin\n",
        "        val_loss = loss1.item()\n",
        "        patience_count = 0\n",
        "    \n",
        "    else:\n",
        "        patience_count += 1\n",
        "\n",
        "    if(patience_count == 10):\n",
        "        patience_count = 0\n",
        "        scheduler.step()\n",
        "\n",
        "print(\"Training Done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Value of training loss for epoch 0 is:  0.0024024778977036476\n",
            "Value of validation loss for epoch 0 is:  0.007159412372857332\n",
            "Value of training loss for epoch 1 is:  0.0016586726997047663\n",
            "Value of validation loss for epoch 1 is:  0.0068598524667322636\n",
            "Value of training loss for epoch 2 is:  0.0010673003271222115\n",
            "Value of validation loss for epoch 2 is:  0.006701030768454075\n",
            "Value of training loss for epoch 3 is:  0.0006227506091818213\n",
            "Value of validation loss for epoch 3 is:  0.0066785383969545364\n",
            "Value of training loss for epoch 4 is:  0.0003211184812244028\n",
            "Value of validation loss for epoch 4 is:  0.006765737198293209\n",
            "Value of training loss for epoch 5 is:  0.00015725877892691642\n",
            "Value of validation loss for epoch 5 is:  0.0068530915305018425\n",
            "Value of training loss for epoch 6 is:  0.00011097300739493221\n",
            "Value of validation loss for epoch 6 is:  0.006891819182783365\n",
            "Value of training loss for epoch 7 is:  0.00010885746451094747\n",
            "Value of validation loss for epoch 7 is:  0.006901474203914404\n",
            "Value of training loss for epoch 8 is:  0.00010884565563173965\n",
            "Value of validation loss for epoch 8 is:  0.006903386674821377\n",
            "Value of training loss for epoch 9 is:  0.00010885551455430686\n",
            "Value of validation loss for epoch 9 is:  0.006904213689267635\n",
            "Value of training loss for epoch 10 is:  0.00010885896335821599\n",
            "Value of validation loss for epoch 10 is:  0.006904972717165947\n",
            "Value of training loss for epoch 11 is:  0.0001088614808395505\n",
            "Value of validation loss for epoch 11 is:  0.00690503790974617\n",
            "Value of training loss for epoch 12 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 12 is:  0.00690503790974617\n",
            "Value of training loss for epoch 13 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 13 is:  0.00690503790974617\n",
            "Adjusting learning rate of group 0 to 5.0000e-05.\n",
            "Value of training loss for epoch 14 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 14 is:  0.00690503790974617\n",
            "Value of training loss for epoch 15 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 15 is:  0.00690503790974617\n",
            "Value of training loss for epoch 16 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 16 is:  0.00690503790974617\n",
            "Value of training loss for epoch 17 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 17 is:  0.00690503790974617\n",
            "Value of training loss for epoch 18 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 18 is:  0.00690503790974617\n",
            "Value of training loss for epoch 19 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 19 is:  0.00690503790974617\n",
            "Value of training loss for epoch 20 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 20 is:  0.00690503790974617\n",
            "Value of training loss for epoch 21 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 21 is:  0.00690503790974617\n",
            "Value of training loss for epoch 22 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 22 is:  0.00690503790974617\n",
            "Value of training loss for epoch 23 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 23 is:  0.00690503790974617\n",
            "Adjusting learning rate of group 0 to 2.5000e-05.\n",
            "Value of training loss for epoch 24 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 24 is:  0.00690503790974617\n",
            "Value of training loss for epoch 25 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 25 is:  0.00690503790974617\n",
            "Value of training loss for epoch 26 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 26 is:  0.00690503790974617\n",
            "Value of training loss for epoch 27 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 27 is:  0.00690503790974617\n",
            "Value of training loss for epoch 28 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 28 is:  0.00690503790974617\n",
            "Value of training loss for epoch 29 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 29 is:  0.00690503790974617\n",
            "Value of training loss for epoch 30 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 30 is:  0.00690503790974617\n",
            "Value of training loss for epoch 31 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 31 is:  0.00690503790974617\n",
            "Value of training loss for epoch 32 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 32 is:  0.00690503790974617\n",
            "Value of training loss for epoch 33 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 33 is:  0.00690503790974617\n",
            "Adjusting learning rate of group 0 to 1.2500e-05.\n",
            "Value of training loss for epoch 34 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 34 is:  0.00690503790974617\n",
            "Value of training loss for epoch 35 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 35 is:  0.00690503790974617\n",
            "Value of training loss for epoch 36 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 36 is:  0.00690503790974617\n",
            "Value of training loss for epoch 37 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 37 is:  0.00690503790974617\n",
            "Value of training loss for epoch 38 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 38 is:  0.00690503790974617\n",
            "Value of training loss for epoch 39 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 39 is:  0.00690503790974617\n",
            "Value of training loss for epoch 40 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 40 is:  0.00690503790974617\n",
            "Value of training loss for epoch 41 is:  0.00010886278323596343\n",
            "Value of validation loss for epoch 41 is:  0.00690503790974617\n",
            "Value of training loss for epoch 42 is:  0.00010886278323596343\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-270054e4df4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Value of training loss for epoch {i} is: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8Fa3KPr0GVBR",
        "outputId": "d2bdf5a7-1ba1-4e4e-ee94-3d840137e1c2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_acc = np.array(val_acc)\n",
        "x_axis = [i for i in range(0,200)]\n",
        "plt.plot(x_axis,val_acc)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbedxyM21Tho",
        "outputId": "4b6fbf7d-131b-423f-ae8c-6609f7ae0a05"
      },
      "source": [
        "pred_test = model_fin(data1,x_test_fin).flatten()\n",
        "y_test_fin = y_test_fin.float()\n",
        "loss1 = loss_func(pred_test,y_test_fin)\n",
        "test_loss = loss1.item()\n",
        "print(\"Loss on test data is: \",loss1.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on test data is:  0.008047588169574738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrMqRzg0jo1Y"
      },
      "source": [
        "PGNN attempt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsSmJWx_jrX8"
      },
      "source": [
        "import random\n",
        "import networkx as nx\n",
        "import multiprocessing as mp\n",
        "\n",
        "class PGNN_layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim,dist_trainable=True):\n",
        "        super(PGNN_layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.dist_trainable = dist_trainable\n",
        "\n",
        "        if self.dist_trainable:\n",
        "            self.dist_compute = Nonlinear(1, output_dim, 1)\n",
        "\n",
        "        self.linear_hidden = nn.Linear(input_dim*2, output_dim)\n",
        "        self.linear_out_position = nn.Linear(output_dim,1)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data = nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    def forward(self, feature, dists_max, dists_argmax):\n",
        "        if self.dist_trainable:\n",
        "            dists_max = self.dist_compute(dists_max.unsqueeze(-1)).squeeze()\n",
        "\n",
        "        subset_features = feature[dists_argmax.flatten(), :]\n",
        "        subset_features = subset_features.reshape((dists_argmax.shape[0], dists_argmax.shape[1],\n",
        "                                                   feature.shape[1]))\n",
        "        messages = subset_features * dists_max.unsqueeze(-1)\n",
        "\n",
        "        self_feature = feature.unsqueeze(1).repeat(1, dists_max.shape[1], 1)\n",
        "        messages = torch.cat((messages, self_feature), dim=-1)\n",
        "\n",
        "        messages = self.linear_hidden(messages).squeeze()\n",
        "        messages = self.act(messages) # n*m*d\n",
        "\n",
        "        out_position = self.linear_out_position(messages).squeeze(-1)  # n*m_out\n",
        "        out_structure = torch.mean(messages, dim=1)  # n*d\n",
        "\n",
        "        return out_position, out_structure\n",
        "\n",
        "\n",
        "### Non linearity\n",
        "class Nonlinear(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Nonlinear, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data = nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PGNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, feature_dim, hidden_dim, output_dim,\n",
        "                 feature_pre=True, layer_num=2, dropout=True, **kwargs):\n",
        "        super(PGNN, self).__init__()\n",
        "        self.feature_pre = feature_pre\n",
        "        self.layer_num = layer_num\n",
        "        self.dropout = dropout\n",
        "        if layer_num == 1:\n",
        "            hidden_dim = output_dim\n",
        "        if feature_pre:\n",
        "            self.linear_pre = nn.Linear(input_dim, feature_dim)\n",
        "            self.conv_first = PGNN_layer(feature_dim, hidden_dim)\n",
        "        else:\n",
        "            self.conv_first = PGNN_layer(input_dim, hidden_dim)\n",
        "        if layer_num>1:\n",
        "            self.conv_hidden = nn.ModuleList([PGNN_layer(hidden_dim, hidden_dim) for i in range(layer_num - 2)])\n",
        "            self.conv_out = PGNN_layer(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        if self.feature_pre:\n",
        "            x = self.linear_pre(x)\n",
        "        x_position, x = self.conv_first(x, data.dists_max, data.dists_argmax)\n",
        "        if self.layer_num == 1:\n",
        "            return x_position\n",
        "        # x = F.relu(x) # Note: optional!\n",
        "        if self.dropout:\n",
        "            x = F.dropout(x, training=self.training)\n",
        "        for i in range(self.layer_num-2):\n",
        "            _, x = self.conv_hidden[i](x, data.dists_max, data.dists_argmax)\n",
        "            # x = F.relu(x) # Note: optional!\n",
        "            if self.dropout:\n",
        "                x = F.dropout(x, training=self.training)\n",
        "        x_position, x = self.conv_out(x, data.dists_max, data.dists_argmax)\n",
        "        x_position = F.normalize(x_position, p=2, dim=-1)\n",
        "        return x_position\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data1 = dataset[0].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aR3A3Yqr4GC"
      },
      "source": [
        "def get_random_anchorset(n,c=0.5):\n",
        "    m = int(np.log2(n))\n",
        "    copy = int(c*m)\n",
        "    anchorset_id = []\n",
        "    for i in range(m):\n",
        "        anchor_size = int(n/np.exp2(i + 1))\n",
        "        for j in range(copy):\n",
        "            anchorset_id.append(np.random.choice(n,size=anchor_size,replace=False))\n",
        "    return anchorset_id\n",
        "\n",
        "def get_dist_max(anchorset_id, dist, device):\n",
        "    dist_max = torch.zeros((dist.shape[0],len(anchorset_id))).to(device)\n",
        "    dist_argmax = torch.zeros((dist.shape[0],len(anchorset_id))).long().to(device)\n",
        "    for i in range(len(anchorset_id)):\n",
        "        temp_id = torch.as_tensor(anchorset_id[i], dtype=torch.long)\n",
        "        dist_temp = dist[:, temp_id]\n",
        "        dist_max_temp, dist_argmax_temp = torch.max(dist_temp, dim=-1)\n",
        "        dist_max[:,i] = dist_max_temp\n",
        "        dist_argmax[:,i] = temp_id[dist_argmax_temp]\n",
        "    return dist_max, dist_argmax\n",
        "\n",
        "def single_source_shortest_path_length_range(graph, node_range, cutoff):\n",
        "    dists_dict = {}\n",
        "    for node in node_range:\n",
        "        dists_dict[node] = nx.single_source_shortest_path_length(graph, node, cutoff)\n",
        "    return dists_dict\n",
        "\n",
        "def merge_dicts(dicts):\n",
        "    result = {}\n",
        "    for dictionary in dicts:\n",
        "        result.update(dictionary)\n",
        "    return result\n",
        "\n",
        "def all_pairs_shortest_path_length_parallel(graph,cutoff=None,num_workers=4):\n",
        "    nodes = list(graph.nodes)\n",
        "    random.shuffle(nodes)\n",
        "    if len(nodes)<50:\n",
        "        num_workers = int(num_workers/4)\n",
        "    elif len(nodes)<400:\n",
        "        num_workers = int(num_workers/2)\n",
        "\n",
        "    pool = mp.Pool(processes=num_workers)\n",
        "    results = [pool.apply_async(single_source_shortest_path_length_range,\n",
        "            args=(graph, nodes[int(len(nodes)/num_workers*i):int(len(nodes)/num_workers*(i+1))], cutoff)) for i in range(num_workers)]\n",
        "    output = [p.get() for p in results]\n",
        "    dists_dict = merge_dicts(output)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return dists_dict\n",
        "\n",
        "def precompute_dist_data(edge_index, num_nodes, approximate=0):\n",
        "        '''\n",
        "        Here dist is 1/real_dist, higher actually means closer, 0 means disconnected\n",
        "        :return:\n",
        "        '''\n",
        "        graph = nx.Graph()\n",
        "        edge_list = edge_index.transpose(1,0).tolist()\n",
        "        graph.add_edges_from(edge_list)\n",
        "\n",
        "        n = num_nodes\n",
        "        dists_array = np.zeros((n, n))\n",
        "        # dists_dict = nx.all_pairs_shortest_path_length(graph,cutoff=approximate if approximate>0 else None)\n",
        "        # dists_dict = {c[0]: c[1] for c in dists_dict}\n",
        "        dists_dict = all_pairs_shortest_path_length_parallel(graph,cutoff=approximate if approximate>0 else None)\n",
        "        for i, node_i in enumerate(graph.nodes()):\n",
        "            shortest_dist = dists_dict[node_i]\n",
        "            for j, node_j in enumerate(graph.nodes()):\n",
        "                dist = shortest_dist.get(node_j, -1)\n",
        "                if dist!=-1:\n",
        "                    # dists_array[i, j] = 1 / (dist + 1)\n",
        "                    dists_array[node_i, node_j] = 1 / (dist + 1)\n",
        "        return dists_array\n",
        "\n",
        "def preselect_anchor(data, layer_num=1, anchor_num=32, anchor_size_num=4, device='cpu'):\n",
        "\n",
        "    data.anchor_size_num = anchor_size_num\n",
        "    data.anchor_set = []\n",
        "    data.dists_max = None\n",
        "    data.dists_argmax = None\n",
        "    anchor_num_per_size = anchor_num//anchor_size_num\n",
        "    for i in range(anchor_size_num):\n",
        "        anchor_size = 2**(i+1)-1\n",
        "        anchors = np.random.choice(data.num_nodes, size=(layer_num,anchor_num_per_size,anchor_size), replace=True)\n",
        "        data.anchor_set.append(anchors)\n",
        "    data.anchor_set_indicator = np.zeros((layer_num, anchor_num, data.num_nodes), dtype=int)\n",
        "\n",
        "    anchorset_id = get_random_anchorset(data.num_nodes,c=1)\n",
        "    data.dists_max, data.dists_argmax = get_dist_max(anchorset_id, data.dists, device)\n",
        "    return data\n",
        "\n",
        "data2 = data1\n",
        "data2.dists = torch.tensor(precompute_dist_data(data2.edge_index, data2.num_nodes ))\n",
        "data2 = preselect_anchor(data2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj8B4qD42c2M"
      },
      "source": [
        "class combined_model_2(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = PGNN(1433,100,500, 100)\n",
        "        self.mlp1 = nn.Linear(input_dim,hidden_dim)\n",
        "        self.mlp2 = nn.Linear(hidden_dim,1)\n",
        "\n",
        "    def forward(self,data,x):\n",
        "        embeddings = self.layer1(data)\n",
        "        x1 = torch.index_select(embeddings,0,x.T[0].flatten())\n",
        "        y1 = torch.index_select(embeddings,0,x.T[1].flatten())\n",
        "        store = torch.cat((x1,y1),dim=1)\n",
        "        store = self.mlp1(store)\n",
        "        store = F.relu(store)\n",
        "        store = self.mlp2(store)\n",
        "        store = F.relu(store)\n",
        "        return store\n",
        "\n",
        "model_fin_2 = combined_model_2(242,400)\n",
        "model_fin_2 = model_fin_2.to(device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm_t70nN4iu7",
        "outputId": "8a2359da-1fd5-4b50-af3d-b5381e13b28b"
      },
      "source": [
        "optimizer_2 = torch.optim.Adam(model_fin_2.parameters(), lr=0.0001,weight_decay=1e-5)\n",
        "scheduler_2 = torch.optim.lr_scheduler.ExponentialLR(optimizer_2, gamma=0.5, verbose=True)\n",
        "loss_func = nn.MSELoss()\n",
        "x_train_fin = torch.from_numpy(x_train)\n",
        "x_train_fin = x_train_fin.to(device)\n",
        "y_train_fin = torch.from_numpy(y_train)\n",
        "y_train_fin = y_train_fin.to(device)\n",
        "\n",
        "x_val_fin = torch.from_numpy(x_val)\n",
        "x_val_fin = x_val_fin.to(device)\n",
        "y_val_fin = torch.from_numpy(y_val)\n",
        "y_val_fin = y_val_fin.to(device)\n",
        "\n",
        "x_test_fin = torch.from_numpy(x_test)\n",
        "x_test_fin = x_test_fin.to(device)\n",
        "y_test_fin = torch.from_numpy(y_test)\n",
        "y_test_fin = y_test_fin.to(device)\n",
        "\n",
        "val_loss = None\n",
        "val_acc = []\n",
        "best_model = None\n",
        "patience_count= 0\n",
        "epoch = 100\n",
        "for i in range(0,epoch):\n",
        "    pred_val = model_fin_2(data2,x_train_fin).flatten()\n",
        "    y_train_fin= y_train_fin.float()\n",
        "    loss1 = loss_func(pred_val,y_train_fin)\n",
        "    print(f\"Value of training loss for epoch {i} is: \",loss1.item())  \n",
        "    optimizer_2.zero_grad()  \n",
        "    loss1.backward()\n",
        "    optimizer_2.step()\n",
        "\n",
        "    \n",
        "    pred_val = model_fin_2(data1,x_val_fin).flatten()\n",
        "    y_val_fin = y_val_fin.float()\n",
        "    loss1 = loss_func(pred_val,y_val_fin)\n",
        "    val_acc.append(loss1.item())\n",
        "    print(f\"Value of validation loss for epoch {i} is: \",loss1.item())    \n",
        "    if best_model is None:\n",
        "        best_model = model_fin_2\n",
        "        val_loss = loss1.item()\n",
        "        patience_count = 0\n",
        "\n",
        "    elif loss1.item() < val_loss:\n",
        "        best_model = model_fin_2\n",
        "        val_loss = loss1.item()\n",
        "        patience_count = 0\n",
        "    \n",
        "    else:\n",
        "        patience_count += 1\n",
        "\n",
        "    if(patience_count == 10):\n",
        "        patience_count = 0\n",
        "        scheduler_2.step()\n",
        "\n",
        "print(\"Training Done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    }
  ]
}